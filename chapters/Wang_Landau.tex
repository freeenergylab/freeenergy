% !TeX spellcheck = en_US
% !TeX encoding = UTF-8
\section{Wang--Landau Algorithm\label{Sec:ES:Wang--Landau}}
Wang--Landau algorithm was developed by Wang and Landau in 2001 to accelerate the convergence in calculating the density of states.\cite{WangPRL2001} In conventional Monte Carlo simulation at a certain temperature $T$, the configurations are generated with a probability proportional to the product of the density of states $g(E)$ and the Boltzmann factor $e^{-E/k_BT}$. While Wang-Landau algorithm aims to estimate the density of states $g(E)$ via a random walk in energy space to produce a flat histogram. If a random walk in energy space is performed with a probability proportional to the reciprocal of the density of states $1/g(E)$, a flat histogram can be generated for the energy distribution. This is accomplished by simultaneously modifying the estimated density of states in a systematic way to produce a flat histogram over the allowed range of energy and making the density of states converge to the true value. Note that at the beginning of the random walk, the density of state is normally unknown, so we simply set them to one for all the energies, i.e. $g(E)=1$. Then the random walk in energy space begins by changing the configuration, for instance flipping the spin in Ising model, randomly with a probability
\begin{equation}
    p(E_1\to E_2)=\min\left[\frac{g(E_1)}{g(E_2)},1\right],
\end{equation}
where $E_1$ and $E_2$ are energies of the configurations before and after the change. Each time an energy level $E$ is visited, the corresponding density of states is updated by multiplying the existing value by a modification factor $f>1$, i.e., $g(E)\to g(E)f$. Initially, the modification factor $f$ can be set to a value as large as $f_0=e^1$, which leads to a crazy exploration in the energy space and the walker can quickly cover all energy levels. This random walk keeps on until we have a ``flat'' histogram $H(E)$. At this moment, the energy levels have been swept in a coarse manner and the density of states converges to the true value with an accuracy proportional to $\ln{(f)}$. Now, we reduce the modification factor to a finer one according to some recipe such as $f_1=\sqrt{f_0}$ (any function that monotonically decrease to 1 will do) and reset the histogram $H(E)=0$. Then we begin the next round of random walk with a finer modification factor $f_1$ until the histogram is flat again. This iteration continues with $f_{i+1}=\sqrt{f_i}$ until a pre-selected criterion such as $f_{final}<\exp(10^{-8})=1.00000001$ has been reached. In reality, a perfect ``flatness'' can never be reached. But we can define a ``flat'' histogram to be the condition that the histograms for all the $E$ level is not less than 80\% of the average histogram $\left<H(E)\right>$. The flowchart of the algorithm  is shown below.
\begin{figure}[htbp]
\begin{tikzpicture}
	\tikzstyle{terminator} = [rectangle, draw, text centered, rounded corners, minimum height=2em]
	\tikzstyle{process} = [rectangle, draw, text badly centered, minimum height=2em,align=center]
	\tikzstyle{decision} = [diamond, aspect=3, draw, text centered, minimum height=2em]
	\tikzstyle{data}=[trapezium, draw, text centered, trapezium left angle=60, trapezium right angle=120, minimum height=2em]
	\tikzstyle{connector} = [draw, -latex']
	\node [terminator, fill=blue!20] at (0,-0.3) (Start) {\textbf{Start}};
	\node [terminator, fill=blue!20] at (0,-2.0) (Initialization) {$\mathbf{i=0,\, \mathbf{x}_i=\mathbf{x}0},\, f=e^0,\, \Omega(E)=1,\, H(E)=0 \quad \forall E$};
	\node [process, fill=green!20, text width=10.5cm] at (0,-5.4) (Propagation) {Propose a trial state $\mathbf{x}_t$ \\ generate a random number \mbox{$s\in[0,1]$}
		\begin{equation*}
			\mathbf{x}_{i+1}=\begin{cases}
				\mathbf{x}_t, &\text{if } s<=\min\left[\frac{\Omega(E(\mathbf{x}_i))}{\Omega(E(\mathbf{x}_t))},1\right]\\
				\mathbf{x}_i, &\text{otherwise}
			\end{cases}
		\end{equation*} 
		\mbox{$H(E(\mathbf{x}_{i+1}))=H(E(\mathbf{x}_{i+1}))+1,\quad \Omega(E(\mathbf{x}_{i+1}))=f\Omega(E(\mathbf{x}_{i+1}))$} \\ \mbox{$i=i+1$}};
	\node [decision, fill=blue!20] at (0,-9.1) (Decision_H) {flat $H(E)$?};
	\node [process, fill=green!20] at (0, -11.2) (Update_f) {$f={f}^{1/2},\, H(E)=0$};
	\node [decision, fill=blue!20] at (0,-13.3) (Decision_f) {$f-1<10^{-8}$?};
	\node [terminator, fill=blue!20] at (0,-15.4) (End) {Output $\Omega(E)$};
	
	\path [connector] (Start) -- (Initialization);
	\path [connector] (Initialization) -- (Propagation);
	\path [connector] (Propagation) -- (Decision_H);
	\path [connector] (Decision_H) -- (Update_f);
	\path [connector] (Update_f) -- (Decision_f);
	\path [connector] (Decision_f) -- (End);
	
	\path [connector] (Decision_H) -- +(6.2,0) |- (Propagation);
	\path [connector] (Decision_f) -- +(-6.2,0) |- (Propagation);
	
	\node[draw=none, font=\footnotesize] at (0.3, -10.2) (Yes_flat) {Yes};
	\node[draw=none, font=\footnotesize] at (6.5, -7.2) (No_flat) {No};
	\node[draw=none, font=\footnotesize] at (0.3, -14.4) (Yes_end) {Yes};
	\node[draw=none, font=\footnotesize] at (-6.5, -9.5) (No_end) {No};
	
\end{tikzpicture}
\caption{The Wang--Landau Algorithm}
\end{figure}

This method can be further enhanced by performing, for instance, multiple random walks etc. Besides, the original implementation suffers from convergence difficulty, which originates from the decay scheme of $f$. Belardinelli and Pereyra proposed a new scheme to solve the difficulty, in which $f=\exp{(t^{-1})}$.\cite{BelardinelliPRE2007}

This algorithm was extended by Atchad\'{e} and Liu\cite{AtchadeSS2010}, and by Liang et al\cite{LiangJASA2007}.